<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>9.6. Training &mdash; MLPro Documentations 1.0.0 documentation</title><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" /><link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="9.7. 3rd Party Support" href="07_wrapper.html" />
    <link rel="prev" title="9.5. Scenarios" href="05_scenario.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MLPro Documentations<img src="../../../../_static/logo_mlpro.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Welcome to MLPro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../01_welcome/sub/01_introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../01_welcome/sub/02_getting_started.html">2. Getting Started</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic Functions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../02_basic_functions/mlpro_bf/main.html">7. MLPro-BF - Basic Functions</a></li>
</ul>
<p class="caption"><span class="caption-text">Machine Learning</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../mlpro_sl/main.html">8. MLPro-SL - Supervised Learning</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../main.html">9. MLPro-RL - Reinforcement Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_overview.html">9.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_getstarted.html">9.2. Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_env.html">9.3. Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_agents.html">9.4. Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_scenario.html">9.5. Scenarios</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">9.6. Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_wrapper.html">9.7. 3rd Party Support</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../mlpro_gt/main.html">10. MLPro-GT - Game Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mlpro_oa/main.html">11. MLPro-OA - Online Adaptivity</a></li>
</ul>
<p class="caption"><span class="caption-text">Appendices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../04_appendices/appendix1/main.html">A1 - Example Pool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../04_appendices/appendix2/main.html">A2 - API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../04_appendices/appendix3/main.html">A3 - Project MLPro</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MLPro Documentations</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../main.html"><span class="section-number">9. </span>MLPro-RL - Reinforcement Learning</a> &raquo;</li>
      <li><span class="section-number">9.6. </span>Training</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/fhswf/MLPro/blob/main/doc/docs/content/03_machine_learning/mlpro_rl/sub/06_train.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="training">
<h1><span class="section-number">9.6. </span>Training<a class="headerlink" href="#training" title="Permalink to this headline">Â¶</a></h1>
<p>In RL, the agent and the environment interact over a sequence of time steps.
At each time step, the agent receives an observation of the current state of the environment and selects an action.
The environment then transitions to a new state and returns a reward signal to the agent.
This process continues until some terminal state is reached.</p>
<p>The agent uses the observed state-action-reward sequences to update its policy,
either through model-based methods that estimate the underlying dynamics of the environment,
or model-free methods that directly estimate the value or the policy.
The policy is used to select actions in subsequent interactions with the environment, allowing the agent to learn from its mistakes and improve over time.</p>
<p>In MLPro-RL, a class <strong>RLTraining</strong> inherits the functionality from class <strong>Training</strong> in the basic function level, where the <strong>RLTraining</strong> class are used for training and hyperparameter tuning of RL agents.
We implement episodic training algorithms and make the corresponding extended training data and results as well as the trained agents available in the file system.
In this RL training, we always start with a defined random initial state of the environment and evaluate at each time step whether one of the following three categories is satisfied,</p>
<ol class="arabic simple">
<li><p><strong>Event Success</strong>: This means that the defined target state is reached and the actual episode is ended.</p></li>
<li><p><strong>Event Broken</strong>: This means that the defined target state is no longer reachable and the actual episode is ended.</p></li>
<li><p><strong>Event Timeout</strong>: This means that the maximum training cycles for an episode are reached and the actual episode is ended.</p></li>
</ol>
<p>If none of the events is satisfied, then the training continues. The goal of the training is to maximize the score of the repetitive evaluations.
In this case, <a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/att/howto.rl.att.001.html#howto-rl-att-001"><span class="std std-ref">a stagnation detection functionality</span></a> can be incorporated to avoid a long training time without any more improvements.
The training can be ended, once the stagnation is detected. For more information, you can read Section 4.3 of <a class="reference external" href="https://doi.org/10.1016/j.mlwa.2022.100341">MLPro 1.0 paper</a>.</p>
<p>In MLPro-RL, we simplify the process of setting up an RL scenario and training for both single-agent and multi-agent RL, as shown below:</p>
<ul>
<li><p><strong>Single-Agent Scenario Creation</strong></p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlpro.rl.models</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">class</span> <span class="nc">MyScenario</span><span class="p">(</span><span class="n">Scenario</span><span class="p">):</span>

    <span class="n">C_NAME</span>      <span class="o">=</span> <span class="s1">&#39;MyScenario&#39;</span>

    <span class="k">def</span> <span class="nf">_setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_mode</span><span class="p">,</span> <span class="n">p_ada</span><span class="p">:</span><span class="nb">bool</span><span class="p">,</span> <span class="n">p_logging</span><span class="p">:</span><span class="nb">bool</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Here&#39;s the place to explicitely setup the entire rl scenario. Please bind your env to</span>
<span class="sd">        self._env and your agent to self._agent.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            p_mode              Operation mode of environment (see Environment.C_MODE_*)</span>
<span class="sd">            p_ada               Boolean switch for adaptivity of agent</span>
<span class="sd">            p_logging           Boolean switch for logging functionality</span>
<span class="sd">       &quot;&quot;&quot;</span>

       <span class="c1"># Setup environment</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">_env</span>    <span class="o">=</span> <span class="n">MyEnvironment</span><span class="p">(</span><span class="o">....</span><span class="p">)</span>

       <span class="c1"># Setup an agent with selected policy</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">_agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span>
           <span class="n">p_policy</span><span class="o">=</span><span class="n">MyPolicy</span><span class="p">(</span>
            <span class="n">p_state_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">get_state_space</span><span class="p">(),</span>
            <span class="n">p_action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">get_action_space</span><span class="p">(),</span>
            <span class="o">....</span>
            <span class="p">),</span>
            <span class="o">....</span>
        <span class="p">)</span>

<span class="c1"># Instantiate scenario</span>
<span class="n">myscenario</span>  <span class="o">=</span> <span class="n">MyScenario</span><span class="p">(</span><span class="n">p_scenario</span><span class="o">=</span><span class="n">myscenario</span><span class="p">,</span> <span class="o">....</span><span class="p">)</span>

<span class="c1"># Train agent in scenario</span>
<span class="n">training</span>    <span class="o">=</span> <span class="n">Training</span><span class="p">(</span><span class="o">....</span><span class="p">)</span>
<span class="n">training</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p><strong>Multi-Agent Scenario Creation</strong></p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlpro.rl.models</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">class</span> <span class="nc">MyScenario</span><span class="p">(</span><span class="n">Scenario</span><span class="p">):</span>

    <span class="n">C_NAME</span>      <span class="o">=</span> <span class="s1">&#39;MyScenario&#39;</span>

    <span class="k">def</span> <span class="nf">_setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_mode</span><span class="p">,</span> <span class="n">p_ada</span><span class="p">:</span><span class="nb">bool</span><span class="p">,</span> <span class="n">p_logging</span><span class="p">:</span><span class="nb">bool</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Here&#39;s the place to explicitely setup the entire rl scenario. Please bind your env to</span>
<span class="sd">        self._env and your agent to self._agent.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            p_mode              Operation mode of environment (see Environment.C_MODE_*)</span>
<span class="sd">            p_ada               Boolean switch for adaptivity of agent</span>
<span class="sd">            p_logging           Boolean switch for logging functionality</span>
<span class="sd">       &quot;&quot;&quot;</span>

       <span class="c1"># Setup environment</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">_env</span>    <span class="o">=</span> <span class="n">MyEnvironment</span><span class="p">(</span><span class="o">....</span><span class="p">)</span>

       <span class="c1"># Create an empty mult-agent</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">_agent</span>     <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="o">....</span><span class="p">)</span>

       <span class="c1"># Add Single-Agent #1 with own policy (controlling sub-environment #1)</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">_agent</span><span class="o">.</span><span class="n">add_agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">_agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span>
               <span class="n">p_policy</span><span class="o">=</span><span class="n">MyPolicy</span><span class="p">(</span>
                <span class="n">p_state_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">get_state_space</span><span class="p">()</span><span class="o">.</span><span class="n">spawn</span><span class="p">[</span><span class="o">....</span><span class="p">],</span>
                <span class="n">p_action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">get_action_space</span><span class="p">()</span><span class="o">.</span><span class="n">spawn</span><span class="p">[</span><span class="o">....</span><span class="p">],</span>
                <span class="o">....</span>
                <span class="p">),</span>
                <span class="o">....</span>
            <span class="p">),</span>
            <span class="o">....</span>
        <span class="p">)</span>

       <span class="c1"># Add Single-Agent #2 with own policy (controlling sub-environment #2)</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">_agent</span><span class="o">.</span><span class="n">add_agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="o">....</span><span class="p">)</span>

       <span class="o">....</span>

<span class="c1"># Instantiate scenario</span>
<span class="n">myscenario</span>  <span class="o">=</span> <span class="n">MyScenario</span><span class="p">(</span><span class="n">p_scenario</span><span class="o">=</span><span class="n">myscenario</span><span class="p">,</span> <span class="o">....</span><span class="p">)</span>

<span class="c1"># Train agent in scenario</span>
<span class="n">training</span>    <span class="o">=</span> <span class="n">Training</span><span class="p">(</span><span class="o">....</span><span class="p">)</span>
<span class="n">training</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<p>For better understanding of RL training in MLPro-RL, we provide some references that can be followed, as follows:</p>
<ol class="loweralpha simple">
<li><p><a class="reference external" href="https://ars.els-cdn.com/content/image/1-s2.0-S2665963822001051-mmc2.mp4">A sample application video of MLPro-RL on a UR5 robot</a>,</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/agent/howto.rl.agent.002.html#howto-agent-rl-002"><span class="std std-ref">Howto RL-AGENT-002: Train an Agent with Own Policy</span></a>,</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/agent/howto.rl.agent.004.html#howto-agent-rl-004"><span class="std std-ref">Howto RL-AGENT-004: Train Multi-Agent with Own Policy</span></a>,</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/howto.rl.pp.001.html#howto-pp-rl-001"><span class="std std-ref">Howto RL-PP-001: SB3 Policy on UR5 Environment</span></a>,</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/mb/howto.rl.mb.001.html#howto-mb-rl-001"><span class="std std-ref">Howto RL-MB-001: MBRL on RobotHTM Environment</span></a>, and</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/mb/howto.rl.mb.002.html#howto-mb-rl-002"><span class="std std-ref">Howto RL-MB-002: MBRL with MPC on Grid World Environment</span></a>.</p></li>
</ol>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="05_scenario.html" class="btn btn-neutral float-left" title="9.5. Scenarios" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="07_wrapper.html" class="btn btn-neutral float-right" title="9.7. 3rd Party Support" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 South Westphalia University of Applied Sciences, Germany.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>