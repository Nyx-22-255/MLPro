<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Custom Environments Model &mdash; MLPro Documentations 1.0.0 documentation</title><link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" /><link rel="shortcut icon" href="../../../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/language_data.js"></script>
        <script src="../../../../../_static/clipboard.min.js"></script>
        <script src="../../../../../_static/copybutton.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../index.html" class="icon icon-home"> MLPro Documentations<img src="../../../../../_static/logo_mlpro.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Welcome to MLPro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_welcome/sub/01_introduction.html">5. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_welcome/sub/02_getting_started.html">6. Getting Started</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic Functions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_basic_functions/mlpro_bf/main.html">7. MLPro-BF - Basic Functions</a></li>
</ul>
<p class="caption"><span class="caption-text">Machine Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mlpro_sl/main.html">8. MLPro-SL - Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main.html">9. MLPro-RL - Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mlpro_gt/main.html">10. MLPro-GT - Game Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mlpro_oa/main.html">11. MLPro-OA - Online Adaptivity</a></li>
</ul>
<p class="caption"><span class="caption-text">Appendices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_appendices/appendix1/main.html">A1 - Example Pool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_appendices/appendix2/main.html">A2 - API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_appendices/appendix3/main.html">A3 - Project MLPro</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">MLPro Documentations</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Custom Environments Model</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/fhswf/MLPro/blob/main/doc/docs/content/03_machine_learning/mlpro_rl/sub/env/customenvmodel.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="custom-environments-model">
<h1>Custom Environments Model<a class="headerlink" href="#custom-environments-model" title="Permalink to this headline">Â¶</a></h1>
<img alt="../../../../../_images/MLPro-RL-Env_class_EnvModel_commented.png" src="../../../../../_images/MLPro-RL-Env_class_EnvModel_commented.png" />
<ul>
<li><p><strong>Environment Model Creation</strong></p>
<blockquote>
<div><p>To create environment model, the adaptive function needs to created first. In this case, our adaptive function will
predict the next state of the environment based on provided action.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlpro.sl.pool.afct.afct_pytorch</span> <span class="kn">import</span> <span class="n">TorchAFct</span><span class="p">,</span> <span class="n">TorchBufferElement</span><span class="p">,</span> <span class="n">TorchBuffer</span>

<span class="c1"># Create the adaptive function based on Pytorch adaptive function module.</span>
<span class="k">class</span> <span class="nc">OurStatePredictor</span><span class="p">(</span><span class="n">TorchAFct</span><span class="p">):</span>
    <span class="n">C_NAME</span> <span class="o">=</span> <span class="s2">&quot;Our State Predictor&quot;</span>
    <span class="n">C_BUFFER_CLS</span> <span class="o">=</span> <span class="n">TorchBuffer</span>

    <span class="k">def</span> <span class="nf">_setup_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Setup your neural network</span>
        <span class="c1"># Setup your optimizer</span>
        <span class="c1"># Setup your loss function</span>

    <span class="k">def</span> <span class="nf">_input_preproc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Do something here for pre-processing input</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">something</span>
        <span class="k">return</span> <span class="nb">input</span>

    <span class="k">def</span> <span class="nf">_output_postproc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Do something here for post-processing output</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">something</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_adapt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_input</span><span class="p">:</span> <span class="n">Element</span><span class="p">,</span> <span class="n">p_output</span><span class="p">:</span> <span class="n">Element</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># Define you adaptation or how to update your neural network</span>
        <span class="k">return</span> <span class="kc">True</span>
</pre></div>
</div>
<p>After that, we need to create another class that is inherited from the actual environment module and EnvModel, in this case
RobotHTM. For now, we only use the state transition model. The reward, success and broken model are taken from
the original environment module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlpro.rl.model_env</span> <span class="kn">import</span> <span class="n">EnvModel</span>
<span class="kn">from</span> <span class="nn">mlpro.rl.pool.envs.robotinhtm</span> <span class="kn">import</span> <span class="n">RobotHTM</span>

<span class="k">class</span> <span class="nc">OurEnvModel</span><span class="p">(</span><span class="n">RobotHTM</span><span class="p">,</span> <span class="n">EnvModel</span><span class="p">):</span>
    <span class="n">C_NAME</span> <span class="o">=</span> <span class="s2">&quot;Our Env Model&quot;</span>

    <span class="c1"># Put necessary input argument in initialization</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">p_num_joints</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">p_target_mode</span><span class="o">=</span><span class="s2">&quot;Random&quot;</span><span class="p">,</span>
        <span class="n">p_ada</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">p_logging</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="c1"># Initialize the actual environment to get all environment functionalities, such as</span>
        <span class="c1"># _simulate_reaction, _reset, _compute_reward, _compute_broken and _compute_success</span>
        <span class="n">RobotHTM</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_num_joints</span><span class="o">=</span><span class="n">p_num_joints</span><span class="p">,</span> <span class="n">p_target_mode</span><span class="o">=</span><span class="n">p_target_mode</span><span class="p">)</span>

        <span class="c1"># Setup Adaptive Function</span>
        <span class="n">afct_strans</span> <span class="o">=</span> <span class="n">AFctSTrans</span><span class="p">(</span>
            <span class="n">OurStatePredictor</span><span class="p">,</span>
            <span class="n">p_state_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_space</span><span class="p">,</span>
            <span class="n">p_action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_action_space</span><span class="p">,</span>
            <span class="n">p_threshold</span><span class="o">=</span><span class="mf">1.8</span><span class="p">,</span>
            <span class="n">p_buffer_size</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>
            <span class="n">p_ada</span><span class="o">=</span><span class="n">p_ada</span><span class="p">,</span>
            <span class="n">p_logging</span><span class="o">=</span><span class="n">p_logging</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># In this case set only p_afct_strans, which tells the module to use</span>
        <span class="c1"># _simulate_reaction from the adaptive function instead of from the actual environment</span>
        <span class="c1"># Set to None to use function such as compute_reward, compute_broken and compute_success</span>
        <span class="c1"># from the actual environment</span>
        <span class="n">EnvModel</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">p_observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_space</span><span class="p">,</span>
            <span class="n">p_action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_action_space</span><span class="p">,</span>
            <span class="n">p_latency</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dt</span><span class="p">),</span>
            <span class="n">p_afct_strans</span><span class="o">=</span><span class="n">afct_strans</span><span class="p">,</span>
            <span class="n">p_afct_reward</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">p_afct_success</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">p_afct_broken</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">p_ada</span><span class="o">=</span><span class="n">p_ada</span><span class="p">,</span>
            <span class="n">p_logging</span><span class="o">=</span><span class="n">p_logging</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 South Westphalia University of Applied Sciences, Germany.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>