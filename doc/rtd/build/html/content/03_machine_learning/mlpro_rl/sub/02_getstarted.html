<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>9.2. Getting Started &mdash; MLPro Documentations 1.0.0 documentation</title><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" /><link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="9.3. Environments" href="03_env.html" />
    <link rel="prev" title="9.1. Overview" href="01_overview.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MLPro Documentations<img src="../../../../_static/logo_mlpro.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Welcome to MLPro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../01_welcome/sub/01_introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../01_welcome/sub/02_getting_started.html">2. Getting Started</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic Functions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../02_basic_functions/mlpro_bf/main.html">7. MLPro-BF - Basic Functions</a></li>
</ul>
<p class="caption"><span class="caption-text">Machine Learning</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../mlpro_sl/main.html">8. MLPro-SL - Supervised Learning</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../main.html">9. MLPro-RL - Reinforcement Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_overview.html">9.1. Overview</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">9.2. Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_env.html">9.3. Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_agents.html">9.4. Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_scenario.html">9.5. Scenarios</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_train.html">9.6. Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_wrapper.html">9.7. 3rd Party Support</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../mlpro_gt/main.html">10. MLPro-GT - Game Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mlpro_oa/main.html">11. MLPro-OA - Online Adaptivity</a></li>
</ul>
<p class="caption"><span class="caption-text">Appendices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../04_appendices/appendix1/main.html">A1 - Example Pool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../04_appendices/appendix2/main.html">A2 - API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../04_appendices/appendix3/main.html">A3 - Project MLPro</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MLPro Documentations</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../main.html"><span class="section-number">9. </span>MLPro-RL - Reinforcement Learning</a> &raquo;</li>
      <li><span class="section-number">9.2. </span>Getting Started</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/fhswf/MLPro/blob/main/doc/docs/content/03_machine_learning/mlpro_rl/sub/02_getstarted.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="getting-started">
<h1><span class="section-number">9.2. </span>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">Â¶</a></h1>
<p>Here is a concise series to introduce all users to the MLPro-RL in a practical way, whether you are a first-timer or an experienced MLPro user.</p>
<p>If you are a first-timer, then you can begin with <strong>Section (1) What is MLPro?</strong>.
If you have understood MLPro but not reinforcement learning, then you can jump to <strong>Section (2) What is Reinforcement Learning?</strong>.
If you have experience in both MLPro and reinforcement learning, then you can directly start with Section <strong>(3) What is MLPro-RL?</strong>.
After following this step-by-step guideline, we expect the user understands the MLPro-RL in practice and starts using MLPro-RL.</p>
<dl>
<dt><strong>1. What is MLPro?</strong></dt><dd><p>If you are a first-time user of MLPro, you might wonder what is MLPro.
Therefore, we recommend initially start with understanding MLPro by checking out the following steps:</p>
<ol class="loweralpha simple">
<li><p><a class="reference internal" href="../../../01_welcome/sub/01_introduction.html#target-mlpro-introduction"><span class="std std-ref">MLPro: An Introduction</span></a>,</p></li>
<li><p><a class="reference external" href="https://ars.els-cdn.com/content/image/1-s2.0-S2665963822001051-mmc1.mp4">introduction video of MLPro</a>,</p></li>
<li><p><a class="reference internal" href="../../../01_welcome/sub/02_getting_started.html#target-mlpro-getstarted"><span class="std std-ref">installing and getting started with MLPro</span></a>, and</p></li>
<li><p>optionally <a class="reference external" href="https://doi.org/10.1016/j.simpa.2022.100421">MLPro paper in Software Impact journal</a>.</p></li>
</ol>
</dd>
<dt><strong>2. What is Reinforcement Learning?</strong></dt><dd><p>If you have not dealt with reinforcement learning, we recommend starting to understand at least the basic concept of reinforcement learning.
There are plenty of references, articles, papers, books, or videos on the internet that explains reinforcement learning.
But, for deep understanding, we recommend you to read the book from Sutton and Barto, which is <a class="reference external" href="https://dl.acm.org/doi/10.5555/3312046">Reinforcement Learning: An Introduction</a>.</p>
</dd>
<dt><strong>3. What is MLPro-RL?</strong></dt><dd><p>We expect that you have a basic knowledge of MLPro and reinforcement learning.
Therefore, you need to understand the overview of MLPro-RL by following the steps below:</p>
<ol class="loweralpha simple">
<li><p><a class="reference internal" href="01_overview.html#target-overview-rl"><span class="std std-ref">MLPro-RL introduction page</span></a>, and</p></li>
<li><p><a class="reference external" href="https://doi.org/10.1016/j.mlwa.2022.100341">Section 4 of MLPro 1.0 paper</a>.</p></li>
</ol>
</dd>
<dt><strong>4. Understanding Environment in MLPro-RL</strong></dt><dd><p>First of all, it is important to understand the structure of an environment in MLPro, which can be found in <a class="reference internal" href="03_env.html#target-env-rl"><span class="std std-ref">this page</span></a>.</p>
<p>Then, you can start following some of our howto files related to the environment in MLPro-RL, as follows:</p>
<ol class="loweralpha simple">
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/howto.rl.001.html#howto-rl-001"><span class="std std-ref">Howto RL-001: Reward</span></a>, and</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/env/howto.rl.env.004.html#howto-env-rl-004"><span class="std std-ref">Howto RL-ENV-004: A Random Agent on Double Pendulum Environment</span></a>.</p></li>
</ol>
</dd>
<dt><strong>5. Understanding Agent in MLPro-RL</strong></dt><dd><p>In reinforcement learning, we have two types of agents, such as a single-agent RL or a multi-agent RL. Both of the types are covered by MLPro-RL.
To understand the different possibilities of an agent in MLPro, you can visit <a class="reference internal" href="04_agents.html#target-agents-rl"><span class="std std-ref">this page</span></a>.</p>
<p>Then, you need to understand how to set up a single-agent and a multi-agent RL in MLPro-RL by following these examples:</p>
<ol class="loweralpha simple">
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/agent/howto.rl.agent.001.html#howto-agent-rl-001"><span class="std std-ref">Howto RL-AGENT-001: Run an Agent with Own Policy</span></a>, and</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/agent/howto.rl.agent.003.html#howto-agent-rl-003"><span class="std std-ref">Howto RL-AGENT-003: Run Multi-Agent with Own Policy</span></a>.</p></li>
</ol>
</dd>
<dt><strong>6. Selecting between Model-Free and Model-Based RL</strong></dt><dd><p>In this section, you need to select your direction of the RL training, whether it is a model-free RL or a model-based RL.
However, firstly, you can pay attention to these two pages, which are <a class="reference internal" href="05_scenario.html#target-scenario-rl"><span class="std std-ref">RL scenario</span></a> and <a class="reference internal" href="06_train.html#target-training-rl"><span class="std std-ref">training</span></a>, before selecting either of the paths below.</p>
<ul>
<li><p>Model-Free Reinforcement Learning</p>
<blockquote>
<div><p>To practice model-free RL in the MLPro-RL package, here are a video and some ready-to-use howto files that can be followed:</p>
<ol class="loweralpha simple">
<li><p><a class="reference external" href="https://ars.els-cdn.com/content/image/1-s2.0-S2665963822001051-mmc2.mp4">A sample application video of MLPro-RL on a UR5 robot</a>,</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/agent/howto.rl.agent.002.html#howto-agent-rl-002"><span class="std std-ref">Howto RL-AGENT-002: Train an Agent with Own Policy</span></a>,</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/agent/howto.rl.agent.004.html#howto-agent-rl-004"><span class="std std-ref">Howto RL-AGENT-004: Train Multi-Agent with Own Policy</span></a>, and</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/howto.rl.pp.001.html#howto-pp-rl-001"><span class="std std-ref">Howto RL-PP-001: SB3 Policy on UR5 Environment</span></a>.</p></li>
</ol>
</div></blockquote>
</li>
<li><p>Model-Based Reinforcement Learning</p>
<blockquote>
<div><p>Model-based RL contains two learning paradigms, such as learning the environment (model-based learning) and utilizing the model (e.g. as an action planner).
To practice model-based RL in the MLPro-RL package, here are a howto file that can be followed:</p>
<ol class="loweralpha simple">
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/mb/howto.rl.mb.001.html#howto-mb-rl-001"><span class="std std-ref">Howto RL-MB-001: MBRL on RobotHTM Environment</span></a>, and</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/mb/howto.rl.mb.002.html#howto-mb-rl-002"><span class="std std-ref">Howto RL-MB-002: MBRL with MPC on Grid World Environment</span></a>.</p></li>
</ol>
</div></blockquote>
</li>
</ul>
</dd>
<dt><strong>7. Additional Guidance</strong></dt><dd><p>After following the previous steps, we hope that you could practice MLPro-RL and start using this subpackage for your RL-related activities.
For more advanced features, we highly recommend you to check out the following howto files:</p>
<ol class="loweralpha simple">
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/agent/howto.rl.agent.005.html#howto-agent-rl-005"><span class="std std-ref">Howto RL-AGENT-005: Train and Reload Single Agent</span></a>,</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/ht/howto.rl.ht.001.html#howto-ht-rl-001"><span class="std std-ref">Howto RL-HT-001: Hyperopt</span></a>,</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/ht/howto.rl.ht.002.html#howto-ht-rl-002"><span class="std std-ref">Howto RL-HT-002: Optuna</span></a>,</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/att/howto.rl.att.001.html#howto-rl-att-001"><span class="std std-ref">Howto RL-ATT-001: Stagnation Detection</span></a>, and</p></li>
<li><p><a class="reference internal" href="../../../04_appendices/appendix1/sub/rl/att/howto.rl.att.002.html#howto-rl-att-002"><span class="std std-ref">Howto RL-ATT-002: SB3 Policy with Stagnation Detection</span></a>.</p></li>
</ol>
</dd>
</dl>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="01_overview.html" class="btn btn-neutral float-left" title="9.1. Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="03_env.html" class="btn btn-neutral float-right" title="9.3. Environments" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 South Westphalia University of Applied Sciences, Germany.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>