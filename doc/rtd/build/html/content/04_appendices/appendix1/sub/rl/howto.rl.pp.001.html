<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Howto RL-PP-001: SB3 Policy on UR5 Environment &mdash; MLPro Documentations 1.0.0 documentation</title><link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" /><link rel="shortcut icon" href="../../../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/language_data.js"></script>
        <script src="../../../../../_static/clipboard.min.js"></script>
        <script src="../../../../../_static/copybutton.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link rel="next" title="Howto RL-AGENT-001: Run an Agent with Own Policy" href="agent/howto.rl.agent.001.html" />
    <link rel="prev" title="Howto RL-001: Reward" href="howto.rl.001.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../index.html" class="icon icon-home"> MLPro Documentations<img src="../../../../../_static/logo_mlpro.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Welcome to MLPro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_welcome/sub/01_introduction.html">5. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_welcome/sub/02_getting_started.html">6. Getting Started</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic Functions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_basic_functions/mlpro_bf/main.html">7. MLPro-BF - Basic Functions</a></li>
</ul>
<p class="caption"><span class="caption-text">Machine Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_machine_learning/mlpro_sl/main.html">8. MLPro-SL - Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_machine_learning/mlpro_rl/main.html">9. MLPro-RL - Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_machine_learning/mlpro_gt/main.html">10. MLPro-GT - Game Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_machine_learning/mlpro_oa/main.html">11. MLPro-OA - Online Adaptivity</a></li>
</ul>
<p class="caption"><span class="caption-text">Appendices</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../main.html">A1 - Example Pool</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../01_howto.bf.html">Basic Functions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../02_howto.rl.html">Reinforcement Learning</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../02_howto.rl.html#elementary-or-uncategorized-topics">Elementary or Uncategorized Topics</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="howto.rl.001.html">Howto RL-001: Reward</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Howto RL-PP-001: SB3 Policy on UR5 Environment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../02_howto.rl.html#agents">Agents</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_howto.rl.html#environments">Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_howto.rl.html#adaptive-environments">Adaptive Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_howto.rl.html#model-based-reinforcement-learning">Model-based Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_howto.rl.html#advanced-training-techniques">Advanced Training Techniques</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_howto.rl.html#wrappers">Wrappers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_howto.rl.html#hyperparameter-tuning-tools">Hyperparameter Tuning Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_howto.rl.html#user-interaction">User Interaction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../03_howto.gt.html">Game Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04_howto.oa.html">Online Adaptivity</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../appendix2/main.html">A2 - API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../appendix3/main.html">A3 - Project MLPro</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">MLPro Documentations</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../main.html">A1 - Example Pool</a> &raquo;</li>
          <li><a href="../02_howto.rl.html">Reinforcement Learning</a> &raquo;</li>
      <li>Howto RL-PP-001: SB3 Policy on UR5 Environment</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/fhswf/MLPro/blob/main/doc/docs/content/04_appendices/appendix1/sub/rl/howto.rl.pp.001.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="howto-rl-pp-001-sb3-policy-on-ur5-environment">
<h1>Howto RL-PP-001: SB3 Policy on UR5 Environment<a class="headerlink" href="#howto-rl-pp-001-sb3-policy-on-ur5-environment" title="Permalink to this headline">¶</a></h1>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Please install the following packages to run this examples properly:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://pypi.org/project/stable-baselines3/">Stable-Baselines3</a></p></li>
<li><p><a class="reference internal" href="../../../../03_machine_learning/mlpro_rl/sub/env/pool/ur5jointcontrol.html#ur5jointcontrol"><span class="std std-ref">RL Environment UR5 Joint Control</span></a></p></li>
<li><p><a class="reference external" href="https://pypi.org/project/numpy/">NumPy</a></p></li>
<li><p><a class="reference external" href="https://pypi.org/project/matplotlib/">Matplotlib</a></p></li>
<li><p><a class="reference external" href="https://pypi.org/project/gym/">OpenAI Gym</a></p></li>
<li><p><a class="reference external" href="https://pypi.org/project/torch/">Pytorch</a></p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="executable-code">
<h2>Executable code<a class="headerlink" href="#executable-code" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## -------------------------------------------------------------------------------------------------</span>
<span class="c1">## -- Project : MLPro - A Synoptic Framework for Standardized Machine Learning Tasks</span>
<span class="c1">## -- Package : mlpro.rl.examples</span>
<span class="c1">## -- Module  : howto_rl_pp_001_train_agent_with_sb3_policy_on_ur5_environment.py</span>
<span class="c1">## -------------------------------------------------------------------------------------------------</span>
<span class="c1">## -- History :</span>
<span class="c1">## -- yyyy-mm-dd  Ver.      Auth.    Description</span>
<span class="c1">## -- 2021-11-18  0.0.0     MRD      Creation</span>
<span class="c1">## -- 2021-11-18  1.0.0     MRD      Initial Release</span>
<span class="c1">## -- 2021-12-07  1.0.1     DA       Refactoring</span>
<span class="c1">## -- 2022-02-11  1.1.0     DA       Special derivate for publication</span>
<span class="c1">## -- 2022-05-23  1.2.0     MRD      Add visualize toggle on UR5JointControl for gazebo GUI</span>
<span class="c1">## -- 2022-06-06  1.2.1     MRD      Add real connection option</span>
<span class="c1">## -- 2022-06-13  1.2.2     MRD      Update possibility to run separate simulator and training</span>
<span class="c1">## --                                setting separate ROS Server IP</span>
<span class="c1">## -- 2022-10-14  1.2.3     SY       Refactoring </span>
<span class="c1">## -- 2022-11-07  1.3.0     DA       Refactoring </span>
<span class="c1">## -------------------------------------------------------------------------------------------------</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Ver. 1.3.0 (2022-11-07)</span>

<span class="sd">This module shows how to use SB3 wrapper to train UR5 robot (derivate for paper).</span>

<span class="sd">You will learn:</span>
<span class="sd">    </span>
<span class="sd">1) How to produce the training results of MLPro 1.0 paper</span>
<span class="sd">    </span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="kn">from</span> <span class="nn">mlpro.rl.models</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">mlpro.rl.pool.envs.ur5jointcontrol</span> <span class="kn">import</span> <span class="n">UR5JointControl</span>
<span class="kn">from</span> <span class="nn">stable_baselines3</span> <span class="kn">import</span> <span class="n">PPO</span>
<span class="kn">from</span> <span class="nn">mlpro.wrappers.sb3</span> <span class="kn">import</span> <span class="n">WrPolicySB32MLPro</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>



<span class="c1"># 1 Implement your own RL scenario</span>
<span class="k">class</span> <span class="nc">ScenarioUR5A2C</span><span class="p">(</span><span class="n">RLScenario</span><span class="p">):</span>
    <span class="n">C_NAME</span> <span class="o">=</span> <span class="s1">&#39;Matrix&#39;</span>

    <span class="k">def</span> <span class="nf">_setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_mode</span><span class="p">,</span> <span class="n">p_ada</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">p_visualize</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">p_logging</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Model</span><span class="p">:</span>
        <span class="c1"># 1.1 Setup environment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_env</span> <span class="o">=</span> <span class="n">UR5JointControl</span><span class="p">(</span>
            <span class="n">p_build</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
            <span class="n">p_real</span><span class="o">=</span><span class="n">p_mode</span><span class="p">,</span>
            <span class="n">p_start_simulator</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">p_start_ur_driver</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="c1"># p_ros_server_ip=&quot;172.19.10.199&quot;,</span>
            <span class="n">p_net_interface</span><span class="o">=</span><span class="s2">&quot;enp0s31f6&quot;</span><span class="p">,</span>
            <span class="n">p_robot_ip</span><span class="o">=</span><span class="s2">&quot;172.19.10.41&quot;</span><span class="p">,</span>
            <span class="c1"># p_reverse_ip=&quot;172.19.10.140&quot;, </span>
            <span class="n">p_visualize</span><span class="o">=</span><span class="n">p_visualize</span><span class="p">,</span> 
            <span class="n">p_logging</span><span class="o">=</span><span class="n">p_logging</span><span class="p">)</span>

        <span class="n">policy_sb3</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span>
            <span class="n">policy</span><span class="o">=</span><span class="s2">&quot;MlpPolicy&quot;</span><span class="p">,</span>
            <span class="n">n_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
            <span class="n">env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">_init_setup_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">policy_wrapped</span> <span class="o">=</span> <span class="n">WrPolicySB32MLPro</span><span class="p">(</span>
            <span class="n">p_sb3_policy</span><span class="o">=</span><span class="n">policy_sb3</span><span class="p">,</span>
            <span class="n">p_cycle_limit</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cycle_limit</span><span class="p">,</span>
            <span class="n">p_observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">get_state_space</span><span class="p">(),</span>
            <span class="n">p_action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">get_action_space</span><span class="p">(),</span>
            <span class="n">p_ada</span><span class="o">=</span><span class="n">p_ada</span><span class="p">,</span>
            <span class="n">p_logging</span><span class="o">=</span><span class="n">p_logging</span><span class="p">)</span>

        <span class="c1"># 1.2 Setup standard single-agent with own policy</span>
        <span class="k">return</span> <span class="n">Agent</span><span class="p">(</span>
            <span class="n">p_policy</span><span class="o">=</span><span class="n">policy_wrapped</span><span class="p">,</span>
            <span class="n">p_envmodel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">p_name</span><span class="o">=</span><span class="s1">&#39;Smith&#39;</span><span class="p">,</span>
            <span class="n">p_ada</span><span class="o">=</span><span class="n">p_ada</span><span class="p">,</span>
            <span class="n">p_logging</span><span class="o">=</span><span class="n">p_logging</span>
        <span class="p">)</span>


<span class="c1"># 2 Train agent in scenario</span>
<span class="n">now</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

<span class="n">training</span> <span class="o">=</span> <span class="n">RLTraining</span><span class="p">(</span>
    <span class="n">p_scenario_cls</span><span class="o">=</span><span class="n">ScenarioUR5A2C</span><span class="p">,</span>
    <span class="n">p_env_mode</span><span class="o">=</span><span class="n">Mode</span><span class="o">.</span><span class="n">C_MODE_SIM</span><span class="p">,</span>
    <span class="n">p_cycle_limit</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
    <span class="n">p_cycles_per_epi_limit</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">p_collect_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">p_collect_actions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">p_collect_rewards</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">p_collect_training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">p_visualize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">p_path</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">()),</span>
    <span class="n">p_logging</span><span class="o">=</span><span class="n">Log</span><span class="o">.</span><span class="n">C_LOG_WE</span><span class="p">)</span>

<span class="n">training</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<img alt="../../../../../_images/ur5simulation2.gif" src="../../../../../_images/ur5simulation2.gif" />
<p>The Gazebo GUI should be the first thing that shows up.
The UR5 robot will move depending on the given action and the training is run.
When the training is done, the logged rewards will be plotted using the matplotlib library.</p>
<p>The plotted figure is not reproducible due to the simulator’s nature of simulating real
world scenario. Although seeds can be set for the random generator, the sampling cannot be
done at the exact same time during different runs. For a more reproducible results,
<a class="reference internal" href="env/howto.rl.env.002.html#howto-env-rl-002"><span class="std std-ref">Howto RL-ENV-002</span></a> is more appropriate.</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="howto.rl.001.html" class="btn btn-neutral float-left" title="Howto RL-001: Reward" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="agent/howto.rl.agent.001.html" class="btn btn-neutral float-right" title="Howto RL-AGENT-001: Run an Agent with Own Policy" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 South Westphalia University of Applied Sciences, Germany.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>