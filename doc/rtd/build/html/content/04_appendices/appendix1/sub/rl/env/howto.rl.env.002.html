<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Howto RL-ENV-002: SB3 Policy on RobotHTM Environment &mdash; MLPro Documentations 1.0.0 documentation</title><link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../_static/custom.css" type="text/css" /><link rel="shortcut icon" href="../../../../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../" src="../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../_static/doctools.js"></script>
        <script src="../../../../../../_static/language_data.js"></script>
        <script src="../../../../../../_static/clipboard.min.js"></script>
        <script src="../../../../../../_static/copybutton.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" />
    <link rel="next" title="Howto RL-ENV-003: SB3 Policy on MultiGeo Environment" href="howto.rl.env.003.html" />
    <link rel="prev" title="Howto RL-ENV-001: SB3 Policy on UR5 Environment" href="howto.rl.env.001.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../index.html" class="icon icon-home"> MLPro Documentations<img src="../../../../../../_static/logo_mlpro.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Welcome to MLPro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_welcome/sub/01_introduction.html">5. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_welcome/sub/02_getting_started.html">6. Getting Started</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic Functions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_basic_functions/mlpro_bf/main.html">7. MLPro-BF - Basic Functions</a></li>
</ul>
<p class="caption"><span class="caption-text">Machine Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_machine_learning/mlpro_sl/main.html">8. MLPro-SL - Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_machine_learning/mlpro_rl/main.html">9. MLPro-RL - Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_machine_learning/mlpro_gt/main.html">10. MLPro-GT - Game Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_machine_learning/mlpro_oa/main.html">11. MLPro-OA - Online Adaptivity</a></li>
</ul>
<p class="caption"><span class="caption-text">Appendices</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../main.html">A1 - Example Pool</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../01_howto.bf.html">Basic Functions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../02_howto.rl.html">Reinforcement Learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../02_howto.rl.html#elementary-or-uncategorized-topics">Elementary or Uncategorized Topics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../02_howto.rl.html#agents">Agents</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../../02_howto.rl.html#environments">Environments</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="howto.rl.env.001.html">Howto RL-ENV-001: SB3 Policy on UR5 Environment</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Howto RL-ENV-002: SB3 Policy on RobotHTM Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="howto.rl.env.003.html">Howto RL-ENV-003: SB3 Policy on MultiGeo Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="howto.rl.env.004.html">Howto RL-ENV-004: A Random Agent on Double Pendulum Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="howto.rl.env.005.html">Howto RL-ENV-005: SB3 Policy on Double Pendulum Environment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../02_howto.rl.html#adaptive-environments">Adaptive Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../02_howto.rl.html#model-based-reinforcement-learning">Model-based Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../02_howto.rl.html#advanced-training-techniques">Advanced Training Techniques</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../02_howto.rl.html#wrappers">Wrappers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../02_howto.rl.html#hyperparameter-tuning-tools">Hyperparameter Tuning Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../02_howto.rl.html#user-interaction">User Interaction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../03_howto.gt.html">Game Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_howto.oa.html">Online Adaptivity</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../appendix2/main.html">A2 - API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../appendix3/main.html">A3 - Project MLPro</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">MLPro Documentations</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../main.html">A1 - Example Pool</a> &raquo;</li>
          <li><a href="../../02_howto.rl.html">Reinforcement Learning</a> &raquo;</li>
      <li>Howto RL-ENV-002: SB3 Policy on RobotHTM Environment</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/fhswf/MLPro/blob/main/doc/docs/content/04_appendices/appendix1/sub/rl/env/howto.rl.env.002.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="howto-rl-env-002-sb3-policy-on-robothtm-environment">
<h1>Howto RL-ENV-002: SB3 Policy on RobotHTM Environment<a class="headerlink" href="#howto-rl-env-002-sb3-policy-on-robothtm-environment" title="Permalink to this headline">¶</a></h1>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h2>
<dl>
<dt>Please install the following packages to run this examples properly:</dt><dd><blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://pypi.org/project/torch/">Pytorch</a></p></li>
<li><p><a class="reference external" href="https://pypi.org/project/stable-baselines3/">Stable-Baselines3</a></p></li>
</ul>
</div></blockquote>
</dd>
</dl>
</div>
<div class="section" id="executable-code">
<h2>Executable code<a class="headerlink" href="#executable-code" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## -------------------------------------------------------------------------------------------------</span>
<span class="c1">## -- Project : MLPro - A Synoptic Framework for Standardized Machine Learning Tasks</span>
<span class="c1">## -- Package : mlpro.rl.examples</span>
<span class="c1">## -- Module  : howto_rl_env_002_train_agent_with_SB3_policy_on_robothtm_environment.py</span>
<span class="c1">## -------------------------------------------------------------------------------------------------</span>
<span class="c1">## -- History :</span>
<span class="c1">## -- yyyy-mm-dd  Ver.      Auth.    Description</span>
<span class="c1">## -- 2021-12-01  0.0.0     MRD      Creation</span>
<span class="c1">## -- 2021-12-01  1.0.0     MRD      First Release</span>
<span class="c1">## -- 2021-12-07  1.0.1     DA       Refactoring</span>
<span class="c1">## -- 2021-12-08  1.0.2     MRD      Add parameter to change the hidden layer of the policy</span>
<span class="c1">## -- 2022-05.30  1.0.3     DA       Refactoring</span>
<span class="c1">## -- 2022-10-13  1.0.4     SY       Refactoring </span>
<span class="c1">## -- 2022-11-01  1.0.5     DA       Refactoring </span>
<span class="c1">## -- 2022-11-07  1.1.0     DA       Refactoring </span>
<span class="c1">## -- 2023-02-02  1.2.0     DA       Refactoring </span>
<span class="c1">## -------------------------------------------------------------------------------------------------</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Ver. 1.2.0 (2023-02-02)</span>

<span class="sd">This module shows how to train a wrapped SB3 policy on MLPro&#39;s native Robothtm environment.</span>

<span class="sd">You will learn:</span>
<span class="sd">    </span>
<span class="sd">1) How to set up a scenario for Robothtm and also with SB3 wrapper</span>

<span class="sd">2) How to run the scenario and train the agent</span>
<span class="sd">    </span>
<span class="sd">3) How to plot from the generated results</span>
<span class="sd">    </span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">mlpro.bf.plot</span> <span class="kn">import</span> <span class="n">DataPlotting</span>
<span class="kn">from</span> <span class="nn">mlpro.rl</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">mlpro.rl.pool.envs.robotinhtm</span> <span class="kn">import</span> <span class="n">RobotHTM</span>
<span class="kn">from</span> <span class="nn">stable_baselines3</span> <span class="kn">import</span> <span class="n">PPO</span>
<span class="kn">from</span> <span class="nn">mlpro.wrappers.sb3</span> <span class="kn">import</span> <span class="n">WrPolicySB32MLPro</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>



<span class="c1"># 1 Implement your own RL scenario</span>
<span class="k">class</span> <span class="nc">ScenarioRobotHTM</span> <span class="p">(</span><span class="n">RLScenario</span><span class="p">):</span>
    <span class="n">C_NAME</span> <span class="o">=</span> <span class="s1">&#39;Matrix&#39;</span>

    <span class="k">def</span> <span class="nf">_setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_mode</span><span class="p">,</span> <span class="n">p_ada</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">p_visualize</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">p_logging</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Model</span><span class="p">:</span>
        <span class="c1"># 1.1 Setup environment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_env</span> <span class="o">=</span> <span class="n">RobotHTM</span><span class="p">(</span><span class="n">p_target_mode</span><span class="o">=</span><span class="s2">&quot;fix&quot;</span><span class="p">,</span> <span class="n">p_visualize</span><span class="o">=</span><span class="n">p_visualize</span><span class="p">,</span> <span class="n">p_logging</span><span class="o">=</span><span class="n">p_logging</span><span class="p">)</span>

        <span class="n">policy_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">activation_fn</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span>
                             <span class="n">net_arch</span><span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">pi</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">vf</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])])</span>

        <span class="n">policy_sb3</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span>
            <span class="n">policy</span><span class="o">=</span><span class="s2">&quot;MlpPolicy&quot;</span><span class="p">,</span>
            <span class="n">n_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="n">env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">_init_setup_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">policy_kwargs</span><span class="o">=</span><span class="n">policy_kwargs</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">policy_wrapped</span> <span class="o">=</span> <span class="n">WrPolicySB32MLPro</span><span class="p">(</span>
            <span class="n">p_sb3_policy</span><span class="o">=</span><span class="n">policy_sb3</span><span class="p">,</span>
            <span class="n">p_cycle_limit</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cycle_limit</span><span class="p">,</span>
            <span class="n">p_observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">get_state_space</span><span class="p">(),</span>
            <span class="n">p_action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">get_action_space</span><span class="p">(),</span>
            <span class="n">p_ada</span><span class="o">=</span><span class="n">p_ada</span><span class="p">,</span>
            <span class="n">p_visualize</span><span class="o">=</span><span class="n">p_visualize</span><span class="p">,</span>
            <span class="n">p_logging</span><span class="o">=</span><span class="n">p_logging</span><span class="p">)</span>

        <span class="c1"># 1.2 Setup standard single-agent with own policy</span>
        <span class="k">return</span> <span class="n">Agent</span><span class="p">(</span>
            <span class="n">p_policy</span><span class="o">=</span><span class="n">policy_wrapped</span><span class="p">,</span>
            <span class="n">p_envmodel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">p_name</span><span class="o">=</span><span class="s1">&#39;Smith&#39;</span><span class="p">,</span>
            <span class="n">p_ada</span><span class="o">=</span><span class="n">p_ada</span><span class="p">,</span>
            <span class="n">p_visualize</span><span class="o">=</span><span class="n">p_visualize</span><span class="p">,</span>
            <span class="n">p_logging</span><span class="o">=</span><span class="n">p_logging</span>
        <span class="p">)</span>



<span class="c1"># 2 Create scenario and start training</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># 2.1 Parameters for demo mode</span>
    <span class="n">cycle_limit</span> <span class="o">=</span> <span class="mi">100000</span>
    <span class="n">adaptation_limit</span> <span class="o">=</span> <span class="mi">150</span>
    <span class="n">stagnation_limit</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">eval_frequency</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">eval_grp_size</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">logging</span> <span class="o">=</span> <span class="n">Log</span><span class="o">.</span><span class="n">C_LOG_WE</span>
    <span class="n">visualize</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">())</span>
    <span class="n">plotting</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">else</span><span class="p">:</span>
    <span class="c1"># 2.2 Parameters for internal unit test</span>
    <span class="n">cycle_limit</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">adaptation_limit</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">stagnation_limit</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">eval_frequency</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">eval_grp_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">logging</span> <span class="o">=</span> <span class="n">Log</span><span class="o">.</span><span class="n">C_LOG_NOTHING</span>
    <span class="n">visualize</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">path</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">plotting</span> <span class="o">=</span> <span class="kc">False</span>


<span class="c1"># 3 Train agent in scenario </span>
<span class="n">training</span> <span class="o">=</span> <span class="n">RLTraining</span><span class="p">(</span>
    <span class="n">p_scenario_cls</span><span class="o">=</span><span class="n">ScenarioRobotHTM</span><span class="p">,</span>
    <span class="n">p_cycle_limit</span><span class="o">=</span><span class="n">cycle_limit</span><span class="p">,</span>
    <span class="n">p_cycles_per_epi_limit</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">p_adaptation_limit</span><span class="o">=</span><span class="n">adaptation_limit</span><span class="p">,</span>
    <span class="n">p_stagnation_limit</span><span class="o">=</span><span class="n">stagnation_limit</span><span class="p">,</span>
    <span class="n">p_eval_frequency</span><span class="o">=</span><span class="n">eval_frequency</span><span class="p">,</span>
    <span class="n">p_eval_grp_size</span><span class="o">=</span><span class="n">eval_grp_size</span><span class="p">,</span>
    <span class="n">p_score_ma_horizon</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">p_path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>
    <span class="n">p_visualize</span><span class="o">=</span><span class="n">visualize</span><span class="p">,</span>
    <span class="n">p_logging</span><span class="o">=</span><span class="n">logging</span>
<span class="p">)</span>

<span class="n">training</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>


<span class="c1"># 4 Create Plotting Class</span>
<span class="k">class</span> <span class="nc">MyDataPlotting</span><span class="p">(</span><span class="n">DataPlotting</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">get_plots</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A function to plot data</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">names</span><span class="p">:</span>
            <span class="n">maxval</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">minval</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">printing</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
                <span class="n">raw</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">label</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">fr_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">frame_id</span><span class="p">[</span><span class="n">name</span><span class="p">]:</span>
                    <span class="n">raw</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">get_values</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">fr_id</span><span class="p">)))</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">printing</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                        <span class="n">maxval</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
                        <span class="n">minval</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">maxval</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">printing</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
                        <span class="n">minval</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">printing</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

                    <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">fr_id</span><span class="p">)</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">minval</span> <span class="o">-</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">minval</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">),</span> <span class="n">maxval</span> <span class="o">+</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">maxval</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">))</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Episode&quot;</span><span class="p">)</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;center left&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">plots</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">plots</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">showing</span><span class="p">:</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>


<span class="c1"># 5 Plotting with MLpro  </span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>  
    <span class="n">data_printing</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Cycle&quot;</span><span class="p">:</span> <span class="p">[</span><span class="kc">False</span><span class="p">],</span>
                     <span class="s2">&quot;Day&quot;</span><span class="p">:</span> <span class="p">[</span><span class="kc">False</span><span class="p">],</span>
                     <span class="s2">&quot;Second&quot;</span><span class="p">:</span> <span class="p">[</span><span class="kc">False</span><span class="p">],</span>
                     <span class="s2">&quot;Microsecond&quot;</span><span class="p">:</span> <span class="p">[</span><span class="kc">False</span><span class="p">],</span>
                     <span class="s2">&quot;Smith&quot;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]}</span>
    
    <span class="n">mem</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">get_results</span><span class="p">()</span><span class="o">.</span><span class="n">ds_rewards</span>
    <span class="n">mem_plot</span> <span class="o">=</span> <span class="n">MyDataPlotting</span><span class="p">(</span><span class="n">mem</span><span class="p">,</span> <span class="n">p_showing</span><span class="o">=</span><span class="n">plotting</span><span class="p">,</span> <span class="n">p_printing</span><span class="o">=</span><span class="n">data_printing</span><span class="p">)</span>
    <span class="n">mem_plot</span><span class="o">.</span><span class="n">get_plots</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<img alt="../../../../../../_images/howto15.png" src="../../../../../../_images/howto15.png" />
<p>After the environment is initiated, the training will run for the specified amount of limits.
However, it is noted that there is no visualization available for this environment.
The training log is stored in the location specified and a figure plot similar to the figure
above will be produced.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: ------------------------------------------------------------------------------
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Training Results of run <span class="m">0</span>
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: ------------------------------------------------------------------------------
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: ------------------------------------------------------------------------------
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Scenario          : RL-Scenario Matrix
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Model             : Agent Smith
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Start <span class="nb">time</span> stamp  : YYYY-MM-DD HH:MM:SS.SSSSSS
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- End <span class="nb">time</span> stamp    : YYYY-MM-DD HH:MM:SS.SSSSSS
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Duration          : <span class="m">0</span>:00:59.209252
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Start cycle id    : <span class="m">0</span>
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- End cycle id      : <span class="m">11999</span>
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Training cycles   : <span class="m">12000</span>
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Evaluation cycles : <span class="m">12500</span>
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Adaptations       : <span class="m">120</span>
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- High score        : -83.8038799825578
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Results stored in : <span class="s2">&quot;C:\Users\%username%\YYYY-MM-DD  HH:MM:SS Training RL&quot;</span>
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Training Episodes : <span class="m">120</span>
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: -- Evaluations       : <span class="m">25</span>
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: ------------------------------------------------------------------------------
YYYY-MM-DD  HH:MM:SS.SSSSSS  W  Results  RL: ------------------------------------------------------------------------------
</pre></div>
</div>
<dl class="simple">
<dt>The local result folder contains the training result files:</dt><dd><ul class="simple">
<li><p>agent_actions.csv</p></li>
<li><p>env_rewards.csv</p></li>
<li><p>env_states.csv</p></li>
<li><p>evaluation.csv</p></li>
<li><p>summary.csv</p></li>
<li><p>trained model.pkl</p></li>
</ul>
</dd>
</dl>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="howto.rl.env.001.html" class="btn btn-neutral float-left" title="Howto RL-ENV-001: SB3 Policy on UR5 Environment" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="howto.rl.env.003.html" class="btn btn-neutral float-right" title="Howto RL-ENV-003: SB3 Policy on MultiGeo Environment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 South Westphalia University of Applied Sciences, Germany.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>